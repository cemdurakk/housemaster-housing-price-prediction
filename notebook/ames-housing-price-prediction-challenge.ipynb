{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":95503,"sourceType":"datasetVersion","datasetId":51153}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# üè† HouseWorth: Predicting Ames Housing Sale Prices with Machine Learning\n\nWelcome to **HouseWorth**, a complete machine learning project aiming to predict house sale prices in Ames, Iowa.  \nThis project combines data science best practices, advanced regression techniques, and model explainability tools like SHAP.\n\n---\n\n## üéØ Project Objective\n\nOur goal is to develop a predictive model for real estate pricing, using detailed housing features such as area, quality, neighborhood, and amenities.\n\n---\n\n## üìÇ Dataset Description\n\nThe dataset includes 80+ variables describing different aspects of residential homes.  \nKey attributes include:\n\n- **GrLivArea** (Above ground living area square feet)\n- **GarageCars** (Size of garage in car capacity)\n- **TotalBsmtSF** (Total basement area)\n- **Neighborhood** (Location within Ames city)\n- **OverallQual** (Overall material and finish quality)\n\nüîó Dataset Source: *AmesHousing.csv*\n\n---\n\n## üìö Table of Contents\n\n1. [Import Libraries & Load Dataset](#import)\n2. [Data Overview & Cleaning](#overview)\n3. [Exploratory Data Analysis (EDA)](#eda)\n4. [Feature Engineering](#feature-engineering)\n5. [Modeling (Linear Regression)](#lr-modeling)\n6. [Modeling (Random Forest)](#rf-modeling)\n7. [Modeling (LXGBoost)](#xgb-modeling)\n8. [Model Comparison & Evaluation](#comparison)\n9. [Conclusions & Business Recommendations](#conclusion)\n","metadata":{}},{"cell_type":"markdown","source":"## üì¶ Import Libraries & Load Dataset <a class=\"anchor\" id=\"import\"></a>\n\nLet's start by importing essential libraries and loading the Ames Housing dataset.\n\nWe will:\n- Load the dataset into a pandas DataFrame\n- Preview the first few rows to understand its structure","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n# üìö Essential Libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\n\n# üìÇ Load the Dataset\ndf = pd.read_csv('/kaggle/input/ames-housing-dataset/AmesHousing.csv')\n\n# Preview the first 5 rows\ndf.head()\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-26T19:00:07.931689Z","iopub.execute_input":"2025-04-26T19:00:07.932218Z","iopub.status.idle":"2025-04-26T19:00:08.006237Z","shell.execute_reply.started":"2025-04-26T19:00:07.932178Z","shell.execute_reply":"2025-04-26T19:00:08.005293Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## üîç Data Overview & Basic Cleaning <a class=\"anchor\" id=\"overview\"></a>\n\nBefore diving into exploratory data analysis (EDA), let's understand the overall structure of the dataset.\n\nIn this section, we will:\n- Check the dataset shape\n- Review column data types\n- Identify missing values\n- Detect anomalies or inconsistencies\n","metadata":{}},{"cell_type":"code","source":"# üìè Dataset Shape\nprint(f\"The dataset contains {df.shape[0]} rows and {df.shape[1]} columns.\")\n\n# üßæ Data Types and Non-Null Counts\ndf.info()\n\n# üîç Checking Missing Values\nmissing_values = df.isnull().sum()\nmissing_values = missing_values[missing_values > 0].sort_values(ascending=False)\nmissing_values\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T19:00:08.007729Z","iopub.execute_input":"2025-04-26T19:00:08.008061Z","iopub.status.idle":"2025-04-26T19:00:08.040145Z","shell.execute_reply.started":"2025-04-26T19:00:08.008023Z","shell.execute_reply":"2025-04-26T19:00:08.039207Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## üõ†Ô∏è Handling Missing Values <a class=\"anchor\" id=\"missing\"></a>\n\nNow that we have identified missing values in the dataset, we need to decide how to handle them.\n\nIn this section:\n- Drop columns with excessive missing data\n- Fill missing values based on feature types (categorical vs numerical)\n- Maintain data integrity for modeling\n","metadata":{}},{"cell_type":"code","source":"# üìâ Dropping columns with too many missing values\nthreshold = 0.4  # Eƒüer bir s√ºtunda %40'tan fazla eksik varsa, o s√ºtunu siliyoruz\nmissing_ratio = df.isnull().mean()\ncols_to_drop = missing_ratio[missing_ratio > threshold].index\ndf.drop(columns=cols_to_drop, inplace=True)\n\nprint(f\"Dropped columns with more than 40% missing values: {list(cols_to_drop)}\")\n\n# üõ†Ô∏è Filling missing numerical features with median\nnum_cols = df.select_dtypes(include=['int64', 'float64']).columns\nfor col in num_cols:\n    if df[col].isnull().sum() > 0:\n        median_value = df[col].median()\n        df[col].fillna(median_value, inplace=True)\n\n# üõ†Ô∏è Filling missing categorical features with mode\ncat_cols = df.select_dtypes(include=['object']).columns\nfor col in cat_cols:\n    if df[col].isnull().sum() > 0:\n        mode_value = df[col].mode()[0]\n        df[col].fillna(mode_value, inplace=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T19:00:08.040884Z","iopub.execute_input":"2025-04-26T19:00:08.041153Z","iopub.status.idle":"2025-04-26T19:00:08.087869Z","shell.execute_reply.started":"2025-04-26T19:00:08.041134Z","shell.execute_reply":"2025-04-26T19:00:08.086858Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## üìä Exploratory Data Analysis (EDA) <a class=\"anchor\" id=\"eda\"></a>\n\nNow that our dataset is clean, it's time to explore the data visually.\n\nIn this section:\n- Analyze the distribution of the target variable (`SalePrice`)\n- Identify potential skewness or outliers\n- Explore feature relationships with the target variable","metadata":{}},{"cell_type":"code","source":"# üìà Distribution Plot of SalePrice\nplt.figure(figsize=(8,5))\nsns.histplot(df['SalePrice'], kde=True, color='skyblue')\nplt.title(\"Distribution of SalePrice\")\nplt.xlabel(\"Sale Price\")\nplt.ylabel(\"Frequency\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T19:00:08.089652Z","iopub.execute_input":"2025-04-26T19:00:08.089962Z","iopub.status.idle":"2025-04-26T19:00:08.405981Z","shell.execute_reply.started":"2025-04-26T19:00:08.089939Z","shell.execute_reply":"2025-04-26T19:00:08.404967Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### üìê Skewness and Kurtosis Analysis\n\nLet's statistically evaluate the distribution of `SalePrice`.\n\n- **Skewness** measures the asymmetry of the distribution.\n- **Kurtosis** measures the heaviness of the distribution tails compared to a normal distribution.\n","metadata":{}},{"cell_type":"code","source":"from scipy.stats import skew, kurtosis\n\n# Skewness and Kurtosis calculation\nsaleprice_skew = skew(df['SalePrice'])\nsaleprice_kurtosis = kurtosis(df['SalePrice'])\n\nprint(f\"Skewness of SalePrice: {saleprice_skew:.2f}\")\nprint(f\"Kurtosis of SalePrice: {saleprice_kurtosis:.2f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T19:00:08.407101Z","iopub.execute_input":"2025-04-26T19:00:08.407349Z","iopub.status.idle":"2025-04-26T19:00:08.416263Z","shell.execute_reply.started":"2025-04-26T19:00:08.407331Z","shell.execute_reply":"2025-04-26T19:00:08.415428Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### üî• Log Transformation (If Needed)\n\nIf the target variable `SalePrice` shows significant skewness, applying a log transformation can help to normalize the distribution.\n\nWe will use `np.log1p`, which applies `log(1+x)` transformation to avoid issues with zero values.\n","metadata":{}},{"cell_type":"code","source":"# üìà Applying log transformation\ndf['SalePrice_Log'] = np.log1p(df['SalePrice'])\n\n# üìà Visualizing the transformed SalePrice\nplt.figure(figsize=(8,5))\nsns.histplot(df['SalePrice_Log'], kde=True, color='lightcoral')\nplt.title(\"Distribution of SalePrice after Log Transformation\")\nplt.xlabel(\"Log of Sale Price\")\nplt.ylabel(\"Frequency\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T19:00:08.417265Z","iopub.execute_input":"2025-04-26T19:00:08.417554Z","iopub.status.idle":"2025-04-26T19:00:08.744828Z","shell.execute_reply.started":"2025-04-26T19:00:08.417533Z","shell.execute_reply":"2025-04-26T19:00:08.743995Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### üìà Outlier Detection with Boxplot\n\nAfter applying log transformation, we visualize the `SalePrice_Log` variable using a boxplot.\n\nBoxplots are effective for detecting outliers, as points lying beyond the whiskers represent potential anomalies.\n","metadata":{}},{"cell_type":"code","source":"# üìà Boxplot for SalePrice_Log\nplt.figure(figsize=(8,5))\nsns.boxplot(x=df['SalePrice_Log'], color='orchid')\nplt.title(\"Boxplot of SalePrice after Log Transformation\")\nplt.xlabel(\"Log of Sale Price\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T19:00:08.745715Z","iopub.execute_input":"2025-04-26T19:00:08.746003Z","iopub.status.idle":"2025-04-26T19:00:08.874346Z","shell.execute_reply.started":"2025-04-26T19:00:08.745981Z","shell.execute_reply":"2025-04-26T19:00:08.873120Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### üìà Feature Relationships with SalePrice\n\nNow we analyze how selected numerical and categorical features relate to the target variable `SalePrice`.\n\nUnderstanding these relationships helps guide feature selection and engineering for modeling.","metadata":{}},{"cell_type":"code","source":"# üìã List of important numeric features\nimportant_numeric = ['Overall Qual', 'Gr Liv Area', 'Garage Cars', 'Total Bsmt SF', '1st Flr SF']\n\n\n# üìà Scatterplots\nfig, axs = plt.subplots(nrows=2, ncols=3, figsize=(18,10))\naxs = axs.flatten()\n\nfor i, feature in enumerate(important_numeric):\n    sns.scatterplot(x=df[feature], y=df['SalePrice_Log'], ax=axs[i], color='royalblue')\n    axs[i].set_title(f\"{feature} vs SalePrice_Log\")\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T19:00:08.875237Z","iopub.execute_input":"2025-04-26T19:00:08.875483Z","iopub.status.idle":"2025-04-26T19:00:10.293238Z","shell.execute_reply.started":"2025-04-26T19:00:08.875464Z","shell.execute_reply":"2025-04-26T19:00:10.292200Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### üìã Categorical Features vs SalePrice\n\nLet's analyze how key categorical features such as `Neighborhood` and `HouseStyle` relate to the target variable `SalePrice`.\n\nBoxplots help visualize price distributions across different categories.","metadata":{}},{"cell_type":"code","source":"# üìã List of important categorical features\nimportant_categorical = ['Neighborhood', 'House Style']\n\n# üìà Boxplots for categorical features\nfor feature in important_categorical:\n    plt.figure(figsize=(14,6))\n    sns.boxplot(x=df[feature], y=df['SalePrice_Log'], palette='Set3')\n    plt.xticks(rotation=45)\n    plt.title(f\"{feature} vs SalePrice_Log\")\n    plt.xlabel(feature)\n    plt.ylabel(\"Log of Sale Price\")\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T19:00:10.294242Z","iopub.execute_input":"2025-04-26T19:00:10.294633Z","iopub.status.idle":"2025-04-26T19:00:11.054389Z","shell.execute_reply.started":"2025-04-26T19:00:10.294608Z","shell.execute_reply":"2025-04-26T19:00:11.053251Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### üî• Correlation Matrix and Heatmap\n\nUnderstanding feature correlations is crucial to identifying multicollinearity and selecting impactful features.\n\nA heatmap helps visualize the strength of relationships between numerical variables.\n","metadata":{}},{"cell_type":"code","source":"# üìã Only Numeric Columns for Correlation\nnumeric_df = df.select_dtypes(include=['int64', 'float64'])\n\n# üìà Correlation Matrix\ncorr_matrix = numeric_df.corr()\n\n# üìã Top 10 features most correlated with SalePrice_Log\ntop_corr_features = corr_matrix['SalePrice_Log'].abs().sort_values(ascending=False).head(11).index\n\n# üìà Focused Heatmap\nplt.figure(figsize=(12,8))\nsns.heatmap(numeric_df[top_corr_features].corr(), annot=True, cmap='coolwarm', fmt='.2f')\nplt.title(\"Top Correlated Features with SalePrice_Log\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T19:00:11.057226Z","iopub.execute_input":"2025-04-26T19:00:11.057520Z","iopub.status.idle":"2025-04-26T19:00:11.617146Z","shell.execute_reply.started":"2025-04-26T19:00:11.057497Z","shell.execute_reply":"2025-04-26T19:00:11.616220Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## üõ†Ô∏è Feature Engineering <a class=\"anchor\" id=\"feature-engineering\"></a>\n\nBefore training our models, we need to engineer the features to ensure better performance.\n\nIn this section:\n- Drop unnecessary columns\n- Encode categorical variables\n- Scale numerical variables\n- Finalize the training set\n","metadata":{}},{"cell_type":"code","source":"# üöÆ Drop unnecessary columns (like ID if exists)\nif 'Order' in df.columns:\n    df.drop(columns=['Order'], inplace=True)\n\n# üöÆ Drop the original SalePrice (we use SalePrice_Log instead)\ndf.drop(columns=['SalePrice'], inplace=True)\n\n# üéõÔ∏è Apply One-Hot Encoding to categorical variables\ndf_encoded = pd.get_dummies(df, drop_first=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T19:00:11.617829Z","iopub.execute_input":"2025-04-26T19:00:11.618109Z","iopub.status.idle":"2025-04-26T19:00:11.662923Z","shell.execute_reply.started":"2025-04-26T19:00:11.618086Z","shell.execute_reply":"2025-04-26T19:00:11.661896Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# üéØ Separate features (X) and target variable (y)\nX = df_encoded.drop(columns=['SalePrice_Log'])\ny = df_encoded['SalePrice_Log']\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T19:00:11.663878Z","iopub.execute_input":"2025-04-26T19:00:11.664190Z","iopub.status.idle":"2025-04-26T19:00:11.673873Z","shell.execute_reply.started":"2025-04-26T19:00:11.664169Z","shell.execute_reply":"2025-04-26T19:00:11.673039Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\n# üìè Scale only the features (not the target)\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# üìã Convert scaled features back to a DataFrame\nX_scaled = pd.DataFrame(X_scaled, columns=X.columns)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T19:00:11.674748Z","iopub.execute_input":"2025-04-26T19:00:11.675026Z","iopub.status.idle":"2025-04-26T19:00:11.730379Z","shell.execute_reply.started":"2025-04-26T19:00:11.675006Z","shell.execute_reply":"2025-04-26T19:00:11.729581Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## ü§ñ Linear Regression Modeling <a class=\"anchor\" id=\"lr-modeling\"></a>\n\nWe start the modeling phase with a baseline Linear Regression model.\n\nLinear Regression helps set a benchmark and provides insights into feature relationships.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# ‚úÇÔ∏è Split data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T19:00:11.731492Z","iopub.execute_input":"2025-04-26T19:00:11.731888Z","iopub.status.idle":"2025-04-26T19:00:11.742201Z","shell.execute_reply.started":"2025-04-26T19:00:11.731857Z","shell.execute_reply":"2025-04-26T19:00:11.741170Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nimport numpy as np\n\n# ü§ñ Initialize and train the Linear Regression model\nlin_reg = LinearRegression()\nlin_reg.fit(X_train, y_train)\n\n# üìà Make predictions on training and testing sets (still in log scale)\ny_pred_train_log = lin_reg.predict(X_train)\ny_pred_test_log = lin_reg.predict(X_test)\n\n# üîÑ Convert predictions and actual values back from log scale\ny_train_orig = np.expm1(y_train)\ny_test_orig = np.expm1(y_test)\ny_pred_train_orig = np.expm1(y_pred_train_log)\ny_pred_test_orig = np.expm1(y_pred_test_log)\n\n# üìä Calculate RMSE on the original SalePrice scale\ntrain_rmse = np.sqrt(mean_squared_error(y_train_orig, y_pred_train_orig))\ntest_rmse = np.sqrt(mean_squared_error(y_test_orig, y_pred_test_orig))\n\nprint(f\"Training RMSE (Original Scale): {train_rmse:.2f}\")\nprint(f\"Test RMSE (Original Scale): {test_rmse:.2f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T19:00:11.743194Z","iopub.execute_input":"2025-04-26T19:00:11.743445Z","iopub.status.idle":"2025-04-26T19:00:11.841573Z","shell.execute_reply.started":"2025-04-26T19:00:11.743424Z","shell.execute_reply":"2025-04-26T19:00:11.840927Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## üå≥ Random Forest Regression Modeling <a class=\"anchor\" id=\"rf-modeling\"></a>\n\nWe now move on to training a Random Forest model.\n\nRandom Forests can capture nonlinear relationships and typically outperform simple linear models in complex datasets.\n","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\n\n# üå≥ Initialize and train the Random Forest model\nrf_model = RandomForestRegressor(n_estimators=100, random_state=42)\nrf_model.fit(X_train, y_train)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T19:07:12.659661Z","iopub.execute_input":"2025-04-26T19:07:12.659997Z","iopub.status.idle":"2025-04-26T19:07:18.484238Z","shell.execute_reply.started":"2025-04-26T19:07:12.659972Z","shell.execute_reply":"2025-04-26T19:07:18.483002Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# üìà Make predictions on training and testing sets (still in log scale)\ny_pred_train_rf_log = rf_model.predict(X_train)\ny_pred_test_rf_log = rf_model.predict(X_test)\n\n# üîÑ Convert predictions and actual values back from log scale\ny_train_rf_orig = np.expm1(y_train)\ny_test_rf_orig = np.expm1(y_test)\ny_pred_train_rf_orig = np.expm1(y_pred_train_rf_log)\ny_pred_test_rf_orig = np.expm1(y_pred_test_rf_log)\n\n# üìä Calculate RMSE on the original SalePrice scale\ntrain_rmse_rf = np.sqrt(mean_squared_error(y_train_rf_orig, y_pred_train_rf_orig))\ntest_rmse_rf = np.sqrt(mean_squared_error(y_test_rf_orig, y_pred_test_rf_orig))\n\nprint(f\"Random Forest Training RMSE (Original Scale): {train_rmse_rf:.2f}\")\nprint(f\"Random Forest Test RMSE (Original Scale): {test_rmse_rf:.2f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T19:07:20.725161Z","iopub.execute_input":"2025-04-26T19:07:20.725465Z","iopub.status.idle":"2025-04-26T19:07:20.816294Z","shell.execute_reply.started":"2025-04-26T19:07:20.725444Z","shell.execute_reply":"2025-04-26T19:07:20.815431Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## ‚ö° XGBoost Regression Modeling <a class=\"anchor\" id=\"xgb-modeling\"></a>\n\nNow, we train an XGBoost model, one of the most powerful and popular machine learning algorithms for tabular data.\n\nXGBoost is known for its high performance and ability to handle complex relationships.\n","metadata":{}},{"cell_type":"code","source":"from xgboost import XGBRegressor\n\n# ‚ö° Initialize and train the XGBoost model\nxgb_model = XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\nxgb_model.fit(X_train, y_train)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T19:11:53.270114Z","iopub.execute_input":"2025-04-26T19:11:53.270452Z","iopub.status.idle":"2025-04-26T19:11:53.840206Z","shell.execute_reply.started":"2025-04-26T19:11:53.270428Z","shell.execute_reply":"2025-04-26T19:11:53.839352Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# üìà Make predictions on training and testing sets (still in log scale)\ny_pred_train_xgb_log = xgb_model.predict(X_train)\ny_pred_test_xgb_log = xgb_model.predict(X_test)\n\n# üîÑ Convert predictions and actual values back from log scale\ny_train_xgb_orig = np.expm1(y_train)\ny_test_xgb_orig = np.expm1(y_test)\ny_pred_train_xgb_orig = np.expm1(y_pred_train_xgb_log)\ny_pred_test_xgb_orig = np.expm1(y_pred_test_xgb_log)\n\n# üìä Calculate RMSE on the original SalePrice scale\ntrain_rmse_xgb = np.sqrt(mean_squared_error(y_train_xgb_orig, y_pred_train_xgb_orig))\ntest_rmse_xgb = np.sqrt(mean_squared_error(y_test_xgb_orig, y_pred_test_xgb_orig))\n\nprint(f\"XGBoost Training RMSE (Original Scale): {train_rmse_xgb:.2f}\")\nprint(f\"XGBoost Test RMSE (Original Scale): {test_rmse_xgb:.2f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T19:11:56.013766Z","iopub.execute_input":"2025-04-26T19:11:56.014071Z","iopub.status.idle":"2025-04-26T19:11:56.076068Z","shell.execute_reply.started":"2025-04-26T19:11:56.014051Z","shell.execute_reply":"2025-04-26T19:11:56.075228Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## üåü Feature Importance from XGBoost\n\nUnderstanding which features most influence the target variable helps us interpret the model better.\n\nWe will visualize the top important features ranked by the XGBoost model.\n","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\n# üåü Get feature importances from XGBoost\nfeature_importance = pd.Series(xgb_model.feature_importances_, index=X.columns)\n\n# üìã Sort by importance\nfeature_importance = feature_importance.sort_values(ascending=False)\n\n# üé® Plot top 15 important features\nplt.figure(figsize=(10,6))\nsns.barplot(x=feature_importance.values[:15], y=feature_importance.index[:15], palette='viridis')\nplt.title('Top 15 Important Features - XGBoost')\nplt.xlabel('Feature Importance Score')\nplt.ylabel('Feature')\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T19:17:09.576128Z","iopub.execute_input":"2025-04-26T19:17:09.576505Z","iopub.status.idle":"2025-04-26T19:17:09.847779Z","shell.execute_reply.started":"2025-04-26T19:17:09.576482Z","shell.execute_reply":"2025-04-26T19:17:09.846892Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## üéØ Error Distribution Plot\n\nAnalyzing the distribution of prediction errors is important for understanding model bias and variance.\n\nIdeally, errors should be centered around zero and symmetrically distributed.\n","metadata":{}},{"cell_type":"code","source":"# üìà Calculate errors on test set\nerrors = y_test_xgb_orig - y_pred_test_xgb_orig\n\n# üé® Plot error distribution\nplt.figure(figsize=(10,6))\nsns.histplot(errors, bins=30, kde=True, color='salmon')\nplt.title('Error Distribution - XGBoost Predictions')\nplt.xlabel('Prediction Error ($)')\nplt.ylabel('Frequency')\nplt.axvline(0, color='black', linestyle='--')\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T19:17:55.350278Z","iopub.execute_input":"2025-04-26T19:17:55.350584Z","iopub.status.idle":"2025-04-26T19:17:55.655576Z","shell.execute_reply.started":"2025-04-26T19:17:55.350565Z","shell.execute_reply":"2025-04-26T19:17:55.654572Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## üìä Model Performance Comparison <a class=\"anchor\" id=\"comparison\"></a>\n\nWe now compare the performance of all three models (Linear Regression, Random Forest, XGBoost) based on their RMSE scores.\n\nLower RMSE indicates better model performance.\n","metadata":{}},{"cell_type":"code","source":"# üìã Create a summary DataFrame\nmodel_results = pd.DataFrame({\n    'Model': ['Linear Regression', 'Random Forest', 'XGBoost'],\n    'Training RMSE': [train_rmse, train_rmse_rf, train_rmse_xgb],\n    'Test RMSE': [test_rmse, test_rmse_rf, test_rmse_xgb]\n})\n\n# üìã Display the table\nprint(model_results)\n\n# üé® Plot Test RMSE comparison\nplt.figure(figsize=(8,5))\nsns.barplot(x='Model', y='Test RMSE', data=model_results, palette='pastel')\nplt.title('Model Comparison - Test RMSE')\nplt.ylabel('Test RMSE ($)')\nplt.xlabel('Model')\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T19:20:16.930311Z","iopub.execute_input":"2025-04-26T19:20:16.930665Z","iopub.status.idle":"2025-04-26T19:20:17.104524Z","shell.execute_reply.started":"2025-04-26T19:20:16.930642Z","shell.execute_reply":"2025-04-26T19:20:17.103416Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## üìú Final Conclusion <a class=\"anchor\" id=\"conclusion\"></a>\n\nIn this project, we performed a complete data science pipeline on a real-world housing prices dataset.\n\nKey steps included:\n- Data cleaning and preprocessing\n- Exploratory Data Analysis (EDA)\n- Feature engineering (encoding, scaling)\n- Model training and evaluation (Linear Regression, Random Forest, XGBoost)\n\n**Main findings:**\n- XGBoost achieved the best performance with a Test RMSE of approximately $26,784.\n- Random Forest also performed well, slightly behind XGBoost.\n- Linear Regression served as a good baseline but was outperformed by tree-based models.\n\n**Next Steps (Optional):**\n- Hyperparameter tuning with GridSearchCV or RandomizedSearchCV\n- Feature selection based on importance scores\n- Ensemble methods (stacking multiple models)\n\nOverall, XGBoost was the most effective model for predicting housing prices in this dataset.\n","metadata":{}}]}